#Gibbs sampling for a Gaussian mixture. FOGM, 2015. Finished Nov. 3, 2015.
#Author: Tom Blazejewski.
using Distributions
using PDMats
using PyPlot

function general_gen_data(K, n, dims)
  #Generates K Gaussians with n points in each. The Gaussians are dims dimensions (usually 2).
  sig = 1.0
  centers = [randn(dims) * 3 for x in 1:K]
  norms = [MvNormal(centers[i], sig) for i in 1:K]
  samples = zeros(n*K, dims)
  #Not sure if most efficient way to generate this, but does the trick.
  for k in 1:K
    for s in 1:n
      println((k-1)*n + s)
      samples[(k-1)*n + s, :] = rand(norms[k])
    end
  end
  return centers, shuffle_rows(samples)
end

function color_generator(n)
  #Will make the gradient from green to red, where n is 0..100.
  r = (255 * n) / 100
  g = (255 * (100 - n)) / 100
  b = 0
  return r, g, b
end

function categorical_to_one_hot(z)
  #We get numbers like 3, 4 and we want to turn it to 1-hot encoding (matrix).
  top_cat = maximum(z) #maximum category, let's assume we start at 1.
  big_array = zeros(Int64, length(z), int(top_cat))
  for val in 1:length(z)
    big_array[val, z[val]] = 1
  end
  return big_array
end

function eq_23(sigma, lambda, n, z, x, k)
  #This is just from class notes.
  z = categorical_to_one_hot(z)
  n_k = sum(z[:,k])

  x_bar_k = sum(z[:,k] .* x, 1) / n_k
  mu_hat_k = ((n_k / sigma^2) / ((n_k / sigma^2) + (1/lambda^2))) .* x_bar_k
  lambda_hat_k = ((n_k / sigma^2) + (1/(lambda^2)))^-1

  my_gauss = MvNormal(vec(mu_hat_k), lambda_hat_k)

  return rand(my_gauss)
end

function log_sum_exp(vec)
  #Trick helps avoid numerical instability.
  the_max = maximum(vec)
  s = 0.0
  for i in 1:length(vec)
    s += exp(vec[i] - the_max)
  end
  return log(s) + the_max
end

function total_prob(x, z_mat, mus, sigma, K)
  centers = MvNormal[]
  for i in 1:K
    push!(centers, MvNormal(vec(mus[i, :]), sigma))
  end
  big_prob = 0
  for i in 1:size(x)[1]
    #center_probs will for each point sum the different centers' probabilities and the probability of
    #being generated by that center. Used to plot "score" of how we're doing.
    center_probs = [log(1/3) + z_mat[i, j] + logpdf(centers[j], vec(x[i, :])) for j in 1:K]
    big_prob += log_sum_exp(center_probs)
  end
  return big_prob
end

function eq_21(x, mus, sigma, K)
  #We consider the probabilities of generating the points given centers and assign points to centers (z).
  centers = MvNormal[]
  for i in 1:K
    push!(centers, MvNormal(vec(mus[i, :]), sigma))
  end
  z = zeros(size(x)[1])
  z_mat = zeros(size(x)[1], K)
  for i in 1:size(x)[1]
    center_probs = [logpdf(centers[j], vec(x[i, :])) for j in 1:K]
    partition = log_sum_exp(center_probs)

    prob_vec = float([exp(a - partition) for a in center_probs])
    z_mat[i, :] = [(a - partition) for a in center_probs]

    to_sample = Categorical(prob_vec)
    z[i] = rand(to_sample)
  end
  return z_mat, z
end

function shuffle_rows(data)
  rows = size(data)[1]
  row_indices = [1:rows;]
  data = data[shuffle(row_indices), :]
  return data
end

function main()
  #Generate data.
  rand_int = int(rand() * 10000)
  srand(rand_int)
  #srand(5900)
  #Keeping track of some interesting random seeds for write-up...
  #srand(761)
  #srand(9561)
  #srand(6698)
  srand(4863)
  #srand(5110) #genuinely doesn't quite get there...
  #srand(4734) #the chosen one!
  #8196 pretty interesting
  #srand(8196)
  #879 is a good general one.
  #srand(61)
  #srand(4395)
  #srand(2471) #is fine for 6 clusters...
  #srand(1634) #pretty exciting with 6.
  #srand(3791)
  #srand(4392)
  #srand(8273)
  #u_1, u_2, u_3, data = gen_data()
  K=3
  num_per = 2000
  centers, data = general_gen_data(K, num_per, 2)
  println("data: ", size(data))
  scatter(data[:,1], data[:,2])
  println("Starting")
  sigma = 1.0
  lambda = 0.5
  #Initiliaze mu's, where our centers start.
  mu = rand(MvNormal(randn(2), 2), K)'

  scatter(mu[:,1], mu[:,2], c="yellow", s=200)
  println(centers)
  for center in centers
    scatter(center[1], center[2], c="white", s=300)
  end
  all_probs = Float64[]

  num_iter = 50
  for count in 1:num_iter
    if count % 1 == 0 && count > 1
      println(count)
      r, g, b=color_generator((count/num_iter) * 100)
      scatter(mu[:,1], mu[:,2], c=@sprintf("#%02X%02X%02X", r, g, b), s=50)
    end
    z_mat, z = eq_21(data, mu, sigma, K)
    push!(all_probs, total_prob(data, z_mat, mu, sigma, K))
    mus = zeros(K, 2)
    for i in 1:K
      mu = eq_23(sigma, lambda, K*num_per, z, data, i)
      mus[i, :] = mu
    end
    mu = mus
  end
  println(mu)
  println("rand_int: ", rand_int)
  close()
  plot(1:num_iter, all_probs)
  title("Negative log-likelihood over sampling iterations")
  xlabel("Number of iterations")
  ylabel("Negative log-likelihood")
end

main()
